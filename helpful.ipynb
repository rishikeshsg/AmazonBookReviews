{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from collections import defaultdict\n",
    "import numpy\n",
    "import gzip\n",
    "import string\n",
    "import re\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import linear_model\n",
    "import nltk\n",
    "from nltk.tokenize import treebank\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from textblob import TextBlob\n",
    "\n",
    "def readGz(f):\n",
    "    for l in gzip.open(f):\n",
    "        yield eval(l)\n",
    "\n",
    "def parseData(fname):\n",
    "    for l in urllib.urlopen(fname):\n",
    "        yield eval(l)\n",
    "def inner(x,y):\n",
    "    return sum([x[i]*y[i] for i in range(len(x))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import exp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posW = opinion_lexicon.positive()\n",
    "negW = opinion_lexicon.negative()\n",
    "score = defaultdict(int)\n",
    "for w in posW:\n",
    "    score[w] = 1\n",
    "for w in negW:\n",
    "    score[w] = -1\n",
    "def demo_liu_hu_lexicon(sentence):\n",
    "    tokenizer = treebank.TreebankWordTokenizer()\n",
    "    pos_words = 0\n",
    "    neg_words = 0\n",
    "    tokenized_sent = [score[word.lower()] for word in tokenizer.tokenize(sentence) ]\n",
    "    #print tokenized_sent\n",
    "    return sum(tokenized_sent)/(1+1.0*len(tokenized_sent))\n",
    "    #print tokenized_sent\n",
    "    #tokenized_sent = [word for word in tokenized_sent if word not in stopwords.words('english')]\n",
    "    '''\n",
    "    #print len(tokenized_sent)\n",
    "    for word in tokenized_sent:\n",
    "        if word in posW:\n",
    "            pos_words += 1\n",
    "        elif word in negW:\n",
    "            neg_words += 1\n",
    "\n",
    "    if pos_words > neg_words:\n",
    "        return 1\n",
    "    elif pos_words < neg_words:\n",
    "        return -1\n",
    "    elif pos_words == neg_words:\n",
    "        return 0\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_liu_hu_lexicon(\"Great new perspective on Ash!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print \"Reading data...\"\n",
    "validate = list(readGz(\"R:/UCSD/Fall'15/CSE 255/Assignment 1/assignment1/validate_100K.json.gz\"))\n",
    "print \"done\"\n",
    "validate = validate[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "done\n",
      "900000\n"
     ]
    }
   ],
   "source": [
    "print \"Reading data...\"\n",
    "data = list(readGz(\"R:/UCSD/Fall'15/CSE 255/Assignment 1/assignment1/train_900000.json.gz\"))\n",
    "print \"done\"\n",
    "print len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_all = data[:]\n",
    "data_small = data[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "print len(data_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "userR = defaultdict(list)\n",
    "itemR = defaultdict(list)\n",
    "userReview = defaultdict(list)\n",
    "itemavg = defaultdict(list)\n",
    "itemT = defaultdict(list)\n",
    "for d in data:\n",
    "    time = d['unixReviewTime']\n",
    "    itemT[d['itemID']].append(time)\n",
    "    #useravg[d['reviewerID']].append(d['rating'])\n",
    "    userReview[d['reviewerID']].append(d['rating'])\n",
    "    itemavg[d['itemID']].append(d['rating'])\n",
    "    if d['helpful']['outOf'] != 0:\n",
    "        userR[d['reviewerID']].append(1.0*d['helpful']['nHelpful']/d['helpful']['outOf'])\n",
    "        itemR[d['itemID']].append(1.0*d['helpful']['nHelpful']/d['helpful']['outOf'])\n",
    "    else:\n",
    "        userR[d['reviewerID']].append(0.0)\n",
    "        itemR[d['itemID']].append(0.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avgLen = 0\n",
    "var = 0\n",
    "for d in data:\n",
    "    avgLen += len(d['reviewText'])\n",
    "    var += pow(len(d['reviewText']),2)\n",
    "avgLen /= len(data)\n",
    "var /= len(data)\n",
    "var -= pow(avgLen,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print avgLen\n",
    "print var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxO = 0\n",
    "avgO = 0\n",
    "count = 0\n",
    "for d in data:\n",
    "    if d['helpful']['outOf'] > 1000:\n",
    "        count += 1\n",
    "        #count += 1\n",
    "        #avgO += d['helpful']['outOf']\n",
    "        #maxO = max(maxO, d['helpful']['outOf'])\n",
    "#print maxO\n",
    "#print avgO/count\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent_feat(text,rating):\n",
    "    blob = TextBlob(text)\n",
    "    sent=0.0\n",
    "    for sentence in blob.sentences:\n",
    "        sent+=sentence.sentiment.polarity\n",
    "\n",
    "    if sent>0:\n",
    "        r=5-rating\n",
    "    else:\n",
    "        r=rating\n",
    "    return abs(sent)*r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n"
     ]
    }
   ],
   "source": [
    "sent_nltk = defaultdict(float)\n",
    "sent_tb = defaultdict(float)\n",
    "sent_nltk_sum = defaultdict(float)\n",
    "sent_tb_sum = defaultdict(float)\n",
    "\n",
    "iter = 0\n",
    "for d in data:\n",
    "    iter += 1\n",
    "    if iter % 10000 == 0:\n",
    "        print iter\n",
    "    #print iter\n",
    "    sent_nltk_sum[(d['reviewerID'], d['itemID'])] = demo_liu_hu_lexicon(d['summary'])\n",
    "    sent_nltk[(d['reviewerID'], d['itemID'])] = demo_liu_hu_lexicon(d['reviewText'])\n",
    "    sent_tb[(d['reviewerID'], d['itemID'])] = sent_feat(d['reviewText'], d['rating'])\n",
    "    sent_tb_sum[(d['reviewerID'], d['itemID'])] = sent_feat(d['summary'], d['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for d in validate:\n",
    "    iter += 1\n",
    "    if iter % 10000 == 0:\n",
    "        print iter\n",
    "    sent_nltk_sum[(d['reviewerID'], d['itemID'])] = demo_liu_hu_lexicon(d['summary'])\n",
    "    sent_nltk[(d['reviewerID'], d['itemID'])] = demo_liu_hu_lexicon(d['reviewText'])\n",
    "    sent_tb[(d['reviewerID'], d['itemID'])] = sent_feat(d['reviewText'], d['rating'])\n",
    "    sent_tb_sum[(d['reviewerID'], d['itemID'])] = sent_feat(d['summary'], d['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "data_h = list(readGz(\"helpful.json.gz\"))\n",
    "print len(data_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for d in data_h:\n",
    "    iter += 1\n",
    "    if iter % 10000 == 0:\n",
    "        print iter\n",
    "    sent_nltk_sum[(d['reviewerID'], d['itemID'])] = demo_liu_hu_lexicon(d['summary'])\n",
    "    sent_nltk[(d['reviewerID'], d['itemID'])] = demo_liu_hu_lexicon(d['reviewText'])\n",
    "    sent_tb[(d['reviewerID'], d['itemID'])] = sent_feat(d['reviewText'], d['rating'])\n",
    "    sent_tb_sum[(d['reviewerID'], d['itemID'])] = sent_feat(d['summary'], d['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for d in validate:\n",
    "    data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = [1.0*d['helpful']['nHelpful']/d['helpful']['outOf'] for d in data if d['helpful']['outOf'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outof = [d['helpful']['outOf'] for d in validate]\n",
    "y_validate = [d['helpful']['nHelpful']  for d in validate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000\n"
     ]
    }
   ],
   "source": [
    "data = data_all[:]\n",
    "print len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature created\n"
     ]
    }
   ],
   "source": [
    "def feature(d):\n",
    "    feat = [1.0]\n",
    "    \n",
    "    words = re.findall(r\"[\\w']+\",d['reviewText'])\n",
    "    feat.append(len(words))\n",
    "    feat.append(d['rating'])\n",
    "    feat.append(d['helpful']['outOf'])\n",
    "    caps = sum([1 for w in words if w.isupper()])\n",
    "    feat.append(caps)\n",
    "    feat.append(len(userR[d['reviewerID']]))\n",
    "    feat.append(len(itemR[d['itemID']]))\n",
    "\n",
    "    \n",
    "    #feat.append(sent_tb[(d['reviewerID'], d['itemID'])])\n",
    "    feat.append(sent_nltk[(d['reviewerID'], d['itemID'])])\n",
    "    \n",
    "    #feat.append(sum(userR[d['reviewerID']])*1.0/len(userR[d['reviewerID']]))\n",
    "    feat.append(len(d['reviewText']))\n",
    "    caps = sum([1 for c in d['reviewText'] if c.isupper()])\n",
    "    feat.append(caps)\n",
    "    \n",
    "    count = lambda l1, l2: len(list(filter(lambda c: c in l2, l1)))\n",
    "    feat.append(count(d['reviewText'], string.punctuation))\n",
    "    feat.append(count(d['summary'], string.punctuation))\n",
    "    \n",
    "    words = re.findall(r\"[\\w']+\",d['summary'])\n",
    "    caps = sum([1 for w in words if w.isupper()])\n",
    "    feat.append(caps)\n",
    "    caps = sum([1 for c in d['summary'] if c.isupper()])\n",
    "    feat.append(caps)\n",
    "    \n",
    "    if d['rating'] == 1.0:\n",
    "        feat.append(1)\n",
    "    else:\n",
    "        feat.append(0)\n",
    "    if d['rating'] == 2.0:\n",
    "        feat.append(1)\n",
    "    else:\n",
    "        feat.append(0)\n",
    "    if d['rating'] == 3.0:\n",
    "        feat.append(1)\n",
    "    else:\n",
    "        feat.append(0)\n",
    "    if d['rating'] == 4.0:\n",
    "        feat.append(1)\n",
    "    else:\n",
    "        feat.append(0)\n",
    "    if len(d['reviewText']) < 100:\n",
    "        feat.append(1)\n",
    "    else:\n",
    "        feat.append(0)\n",
    "    if len(d['reviewText']) > 1000:\n",
    "        feat.append(1)\n",
    "    else:\n",
    "        feat.append(0)\n",
    "    return feat\n",
    "X = [feature(d) for d in data if d['helpful']['outOf'] != 0]\n",
    "X_validate = [feature(d) for d in validate]\n",
    "print \"feature created\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_validate = [feature(d) for d in validate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n",
      "mae validate:  0.54754\n"
     ]
    }
   ],
   "source": [
    "#### theta,residuals,rank,s = numpy.linalg.lstsq(X, y)\n",
    "regr_1 = DecisionTreeRegressor(max_depth=10)#, max_features = 7)\n",
    "regr_1.fit(X, y)\n",
    "#clf = linear_model.LinearRegression()\n",
    "#clf.fit(X,y)\n",
    "#theta = clf.coef_\n",
    "#theta[0] = clf.intercept_\n",
    "\n",
    "#estimator = RandomForestRegressor(n_estimators=100, max_depth=10)\n",
    "#estimator.fit(X,y)\n",
    "print \"fit\"\n",
    "\n",
    "pred = regr_1.predict(X_validate)\n",
    "#pred = estimator.predict(X_validate)\n",
    "#print theta\n",
    "#print residuals\n",
    "\n",
    "mae = 0\n",
    "#mae = sum(abs(y_validate - pred))\n",
    "#'''\n",
    "for i in range(len(y_validate)):\n",
    "    #predict = round(inner(X_validate[i],theta)*outof[i])\n",
    "    predict = round(pred[i]*outof[i])\n",
    "    #predict = pred[i]*outof[i]\n",
    "    mae += abs(y_validate[i] - predict)\n",
    "#'''\n",
    "print \"mae validate: \", mae*1.0/len(y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#X_test =  [feature(d) for d in data_h]\n",
    "test = {}\n",
    "X_test = []\n",
    "for d in data_h:\n",
    "    X_test.append(feature(d))\n",
    "pred_test = regr_1.predict(X_test)\n",
    "\n",
    "predictions = open(\"predictions_Helpful.txt\", 'w')\n",
    "iter = 0\n",
    "for l in open(\"pairs_Helpful.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,i,outOf = l.strip().split('-')\n",
    "    outOf = int(outOf)\n",
    "    pred = int(round(outOf*pred_test[iter]))\n",
    "    predictions.write(u + '-' + i + '-' + str(outOf) + ',' + str(pred) + '\\n')\n",
    "    iter += 1\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_h = 0\n",
    "o = []\n",
    "for d in data_h:\n",
    "    o.append(d['helpful']['outOf'])\n",
    "    max_h = max(max_h, d['helpful']['outOf'])\n",
    "print max_h\n",
    "print sorted(o, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
